{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12780174,"sourceType":"datasetVersion","datasetId":8079791},{"sourceId":12780275,"sourceType":"datasetVersion","datasetId":8079864}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport logging\nfrom functools import partial\nfrom collections import OrderedDict\nfrom einops import rearrange, repeat\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.models.helpers import load_pretrained\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nfrom timm.models.registry import register_model\n\nimport os\nfrom torch.utils.data import Dataset\nimport random\nimport numpy as np\n\nfrom torch.utils.data import DataLoader\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.manifold import TSNE\n\nimport matplotlib\nmatplotlib.use('Agg')  # Use the Agg backend\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport matplotlib.cm as cm\nfrom matplotlib.colors import ListedColormap, LinearSegmentedColormap\n\nimport seaborn as sns\nfrom scipy.stats import gaussian_kde\nfrom scipy.optimize import brentq\nfrom scipy.stats import gaussian_kde\nfrom scipy.ndimage import gaussian_filter1d\nfrom matplotlib.lines import Line2D\nfrom matplotlib.patches import Patch\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\nclass GestureTransformer(nn.Module):\n    def __init__(self, num_frame=101, num_joints=21, in_chans=3, embed_dim_ratio=32, depth=4,\n                 num_heads=8, mlp_ratio=2., qkv_bias=True, qk_scale=None,\n                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.2,  norm_layer=None, num_classes = 0):\n        \"\"\"    ##########hybrid_backbone=None, representation_size=None,\n        Args:\n            num_frame (int, tuple): input frame number\n            num_joints (int, tuple): joints number\n            in_chans (int): number of input channels, 2D joints have 2 channels: (x,y)\n            embed_dim_ratio (int): embedding dimension ratio\n            depth (int): depth of transformer\n            num_heads (int): number of attention heads\n            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n            qkv_bias (bool): enable bias for qkv if True\n            qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n            drop_rate (float): dropout rate\n            attn_drop_rate (float): attention dropout rate\n            drop_path_rate (float): stochastic depth rate\n            norm_layer: (nn.Module): normalization layer\n        \"\"\"\n        super().__init__()\n\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n        embed_dim = embed_dim_ratio * num_joints   #### temporal embed_dim is num_joints * spatial embedding dim ratio\n        out_dim = num_classes\n\n\n        ### spatial patch embedding\n        self.Spatial_patch_to_embedding = nn.Linear(in_chans, embed_dim_ratio)\n        self.Spatial_pos_embed = nn.Parameter(torch.zeros(1, num_joints, embed_dim_ratio))\n\n        self.Temporal_pos_embed = nn.Parameter(torch.zeros(1, num_frame, embed_dim))\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n\n        self.Spatial_blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim_ratio, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n            for i in range(depth)])\n\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n            for i in range(depth)])\n\n        self.Spatial_norm = norm_layer(embed_dim_ratio)\n        self.Temporal_norm = norm_layer(embed_dim)\n\n        ####### A easy way to implement weighted mean\n        self.weighted_mean = torch.nn.Conv1d(in_channels=num_frame, out_channels=1, kernel_size=1)\n\n        self.head = nn.Sequential(\n            nn.LayerNorm(embed_dim),\n            nn.Linear(embed_dim , out_dim),\n        )\n\n\n    def Spatial_forward_features(self, x):\n        b, _, f, p = x.shape  ##### b is batch size, f is number of frames, p is number of joints\n        x = rearrange(x, 'b c f p  -> (b f) p  c', )\n        x = self.Spatial_patch_to_embedding(x)\n        x += self.Spatial_pos_embed\n        x = self.pos_drop(x)\n\n        for blk in self.Spatial_blocks:\n            x = blk(x)\n\n        x = self.Spatial_norm(x)\n        x = rearrange(x, '(b f) w c -> b f (w c)', f=f)\n        return x\n\n    def forward_features(self, x):\n        b  = x.shape[0]\n        x += self.Temporal_pos_embed\n        x = self.pos_drop(x)\n        for blk in self.blocks:\n            x = blk(x)\n\n        x = self.Temporal_norm(x)\n        ##### x size [b, f, emb_dim], then take weighted mean on frame dimension, we only predict 3D pose of the center frame\n        x = self.weighted_mean(x)\n        x = x.view(b, 1, -1)\n        return x\n\n    def forward(self, x):\n        bs, t, w, k, d = x.shape\n        x = x.view(bs * t, w, k, d)\n        x = x.permute(0, 3, 1, 2)\n        bs_temp, _, _, p = x.shape\n        x = self.Spatial_forward_features(x)\n        x = self.forward_features(x)\n        x = self.head(x)\n        out_dim = 45\n        x = x.view(bs, 3, out_dim)\n\n        return x","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-16T18:01:38.556060Z","iopub.execute_input":"2025-08-16T18:01:38.556317Z","iopub.status.idle":"2025-08-16T18:01:50.206931Z","shell.execute_reply.started":"2025-08-16T18:01:38.556293Z","shell.execute_reply":"2025-08-16T18:01:50.206165Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n/usr/local/lib/python3.11/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"class MyCustomDataset(Dataset):\n    def __init__(self, root_dir, mode='train', sliding_window_size=101):\n        self.root_dir = root_dir\n        self.subjects = os.listdir(root_dir)\n        self.frame_data = []  # Stores the information of each frame\n        self.labels = [] # Stores the labels of each frame\n        self.indexvalues = []\n        self.sliding_window_size = sliding_window_size\n        self.class_frame_dict={}\n        self._process_data(mode)\n\n    def __len__(self):\n        return len(self.frame_data)\n\n\n    def __getitem__(self, index):\n        center_frame = self.frame_data[index]\n        anchor_label = self.labels[index]\n        anchor_skeleton_number = center_frame['skeleton_number']\n\n        # Positive sample: Random frame from the same gesture class as anchor\n        positive_frames = self.class_frame_dict[anchor_label]\n        positive_frame = random.choice(positive_frames)\n        positive_index_data = self.frame_data[positive_frame['frame_number']]\n        positive_frame_number = positive_index_data['frame_number']\n        positive_label = self.labels[positive_frame_number]\n        positive_skeleton_number = positive_index_data['skeleton_number']\n\n        # Negative sample: Random frame from a different gesture class\n        # Select a random gesture class that is not the same as the anchor class\n        negative_label = random.choice([label for label in self.class_frame_dict.keys() if label != anchor_label])\n        negative_frames = self.class_frame_dict[negative_label]\n        negative_frame = random.choice(negative_frames)\n        negative_index_data = self.frame_data[negative_frame['frame_number']]\n        negative_frame_number = negative_index_data['frame_number']\n        negative_label = self.labels[negative_frame_number]\n        negative_skeleton_number = negative_index_data['skeleton_number']\n\n        skeleton_start_index = self.indexvalues[anchor_skeleton_number]['start_index']\n        skeleton_end_index = self.indexvalues[anchor_skeleton_number]['end_index']\n\n        positive_start_index = self.indexvalues[positive_skeleton_number]['start_index']\n        positive_end_index = self.indexvalues[positive_skeleton_number]['end_index']\n\n\n        negative_start_index = self.indexvalues[negative_skeleton_number]['start_index']\n        negative_end_index = self.indexvalues[negative_skeleton_number]['end_index']\n\n\n        start_index = index - self.sliding_window_size // 2\n        end_index = index + self.sliding_window_size // 2\n\n\n        pos_start_index = positive_frame_number - self.sliding_window_size // 2\n        pos_end_index = positive_frame_number + self.sliding_window_size // 2\n\n\n        neg_start_index = negative_frame_number - self.sliding_window_size // 2\n        neg_end_index = negative_frame_number + self.sliding_window_size // 2\n\n        if (start_index < skeleton_start_index):\n          start_index = skeleton_start_index\n\n        else:\n          start_index = max(start_index, 0)\n\n        if (end_index > skeleton_end_index):\n          end_index = skeleton_end_index\n\n\n        if (pos_start_index < positive_start_index):\n          pos_start_index = positive_start_index\n\n        else:\n          pos_start_index = max(pos_start_index, 0)\n\n        if (pos_end_index > positive_end_index):\n          pos_end_index = positive_end_index\n\n\n        if (neg_start_index < negative_start_index):\n          neg_start_index = negative_start_index\n\n        else:\n          neg_start_index = max(neg_start_index, 0)\n\n        if (neg_end_index > negative_end_index):\n          neg_end_index = negative_end_index\n\n        anchor = []\n\n        for i in range(start_index, end_index + 1):\n\n          frame_data = self.frame_data[i]\n          joint_coordinates = frame_data['joint_coordinates']\n          anchor.append(joint_coordinates)\n\n        while len(anchor) < self.sliding_window_size:\n          anchor.append([0.0] * 63)  # Assuming 21 joints with (x, y, z) coordinates\n\n\n        anchor = torch.tensor(anchor, dtype=torch.float32)\n        anchor = anchor.view(self.sliding_window_size, 21, 3)\n\n        positive = []\n\n        for i in range(pos_start_index, pos_end_index + 1):\n\n          frame_data = self.frame_data[i]\n          joint_coordinates = frame_data['joint_coordinates']\n          positive.append(joint_coordinates)\n\n        while len(positive) < self.sliding_window_size:\n          positive.append([0.0] * 63)  # Assuming 21 joints with (x, y, z) coordinates\n\n\n        positive = torch.tensor(positive, dtype=torch.float32)\n        positive = positive.view(self.sliding_window_size, 21, 3)\n\n        negative = []\n\n        for i in range(neg_start_index, neg_end_index + 1):\n\n          frame_data = self.frame_data[i]\n          joint_coordinates = frame_data['joint_coordinates']\n          negative.append(joint_coordinates)\n\n        while len(negative) < self.sliding_window_size:\n          negative.append([0.0] * 63)  # Assuming 21 joints with (x, y, z) coordinates\n\n\n        negative = torch.tensor(negative, dtype=torch.float32)\n        negative = negative.view(self.sliding_window_size, 21, 3)\n\n        # Create a tensor with three dimensions: anchor, positive, and negative samples\n        sample = torch.stack([anchor, positive, negative])\n\n        return sample, anchor_label\n\n\n\n    def _process_data(self, mode):\n        frame_counter = 0  # Counter for sequential frame numbering\n        skeleton_temp=0\n        indices = []\n        class_frames_dict = {}  # Dictionary to store frames by gesture class\n\n        for subject in self.subjects:\n            subject_dir = os.path.join(self.root_dir, subject)\n\n            if mode == 'train':\n                data_dir = os.path.join(subject_dir, 'TrainingData')\n            elif mode == 'test':\n                data_dir = os.path.join(subject_dir, 'TestingData')\n            else:\n                raise ValueError(\"Invalid mode. Mode should be 'train' or 'test'.\")\n\n            action_folders = os.listdir(subject_dir)\n\n            for action_folder in action_folders:\n                action_dir = os.path.join(subject_dir, action_folder)\n                number_folders = os.listdir(action_dir)\n\n                for number_folder in number_folders:\n                    number_dir = os.path.join(action_dir, number_folder)\n                    skeleton_file = os.path.join(number_dir, \"skeleton.txt\")\n\n                    # Read and process the skeleton.txt file to extract frame-level data\n                    frames, labels , start , end= self._read_skeleton_file(skeleton_file, frame_counter, action_folder , skeleton_temp)\n\n                    indices.append({\n                    'start_index': start,\n                    'end_index': end,\n\n                    })\n                    # Extend the frame_data list with the frame-level data\n                    self.frame_data.extend(frames)\n                    self.labels.extend(labels)\n\n                    # Update the frame counter based on the number of frames in the current skeleton file\n                    frame_counter += len(frames)\n                    skeleton_temp = skeleton_temp + 1\n\n        self.indexvalues.extend(indices)\n\n        # Store frames in class-specific lists\n        for label, frame in zip(self.labels, self.frame_data):\n          if label not in class_frames_dict:\n            class_frames_dict[label] = []\n          class_frames_dict[label].append(frame)\n        self.class_frame_dict.update(class_frames_dict)\n\n\n    def _read_skeleton_file(self, skeleton_file, frame_counter, action , skeleton_temp):\n        frames = []\n        labels = []\n        start_ind = frame_counter\n        temp_index = 0\n\n        # Read and parse the skeleton.txt file to extract frame-level data\n        with open(skeleton_file, 'r') as f:\n            for line in f:\n                # Parse the line and extract frame number, joint coordinates, and label\n                frame_info = line.strip().split(' ')\n                frame_number = frame_counter + int(frame_info[0])\n                joint_coordinates = [float(coord) for coord in frame_info[1:]]\n\n                label = action\n                end_ind = frame_number\n\n                # Append the frame data and label to the respective lists\n                frames.append({\n                    'frame_number': frame_number,\n                    'joint_coordinates': joint_coordinates,\n                    'skeleton_number' : skeleton_temp,\n\n                })\n                labels.append(label)\n\n                temp_index = frame_number\n\n        return frames, labels, start_ind, temp_index\n\n\n\nclass TripletLossCosine(nn.Module):\n    def __init__(self, margin=1.0):\n        super(TripletLossCosine, self).__init__()\n        self.margin = margin\n\n    def forward(self, anchor, positive, negative):\n        # Cosine similarity\n        similarity_positive = nn.functional.cosine_similarity(anchor, positive)\n        similarity_negative = nn.functional.cosine_similarity(anchor, negative)\n\n        # Calculate triplet loss\n        triplet_loss = torch.relu(similarity_negative - similarity_positive + self.margin)\n\n        return torch.mean(triplet_loss)\n\n\nclass TripletLoss(nn.Module):\n    def __init__(self, margin=1.0):\n        super(TripletLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, anchor, positive, negative):\n\n        distance_positive = torch.sqrt(torch.sum((anchor - positive) ** 2, dim=1))\n        distance_negative = torch.sqrt(torch.sum((anchor - negative) ** 2, dim=1))\n        losses = torch.relu(distance_positive - distance_negative + self.margin)\n\n        triplet_loss = torch.mean(losses)\n\n\n        return triplet_loss\n\n\n\ndef compute_distance(embeddings1, embeddings2, distance_metric='euclidean'):\n    if distance_metric == 'euclidean':\n        distances = torch.norm(embeddings1 - embeddings2, p=2, dim=2)\n\n    elif distance_metric == 'cosine':\n        similarity = torch.nn.functional.cosine_similarity(embeddings1, embeddings2, dim=2)\n        distances = 1 - similarity  # Convert similarity to distance\n    else:\n        raise ValueError(\"Unsupported distance metric. Choose 'euclidean' or 'cosine'.\")\n\n    return distances\n\n\n\ndef calculate_accuracy(anchor_positive_distances, anchor_negative_distances, threshold):\n    # Assuming anchor-positive distances are similar pairs and anchor-negative distances are dissimilar pairs\n    similar_pairs = anchor_positive_distances <= threshold\n    dissimilar_pairs = anchor_negative_distances > threshold\n\n    # True positives: Similar pairs correctly classified as similar\n    true_positives = np.sum(similar_pairs)\n\n    # True negatives: Dissimilar pairs correctly classified as dissimilar\n    true_negatives = np.sum(dissimilar_pairs)\n\n    # False positives: Dissimilar pairs incorrectly classified as similar\n    false_positives = np.sum(anchor_negative_distances <= threshold)\n\n    # False negatives: Similar pairs incorrectly classified as dissimilar\n    false_negatives = np.sum(anchor_positive_distances > threshold)\n\n    # Calculate accuracy\n    accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n\n    return accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T18:01:50.208664Z","iopub.execute_input":"2025-08-16T18:01:50.209376Z","iopub.status.idle":"2025-08-16T18:01:50.231208Z","shell.execute_reply.started":"2025-08-16T18:01:50.209357Z","shell.execute_reply":"2025-08-16T18:01:50.230374Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Set the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\ntrain_root_dir = '/kaggle/input/fphadataset/FPHADataset/TrainingData'\ntest_root_dir = '/kaggle/input/fphadataset/FPHADataset/TestingData'\n\n\n# Create an instance of the training dataset\ntrain_dataset = MyCustomDataset(train_root_dir, mode='train')\n\n# Create an instance of the testing dataset\ntest_dataset = MyCustomDataset(test_root_dir, mode='test')\n\n# Set the batch size\nbatch_size = 32\n\n# Create data loaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\n\ntrain_dataset_length = len(train_dataset)\n\ntest_dataset_length = len(test_dataset)\n\n# Set the number of output classes\nnum_classes = 45 # Replace with the embedding dimension value\nemb_dim = num_classes\n# Set the hyperparameters\n#Sliding window size\nnum_frame =101\nnum_joints = 21\nin_chans = 3\n# Embedding Dimension\nembed_dim_ratio = 64\ndepth = 4\nnum_heads = 8\nmlp_ratio = 2.\nqkv_bias = True\nqk_scale = None\ndrop_rate = 0.\nattn_drop_rate = 0.\ndrop_path_rate = 0.2\n\n# Create an instance of the PoseTransformer model\nmodel = GestureTransformer(num_frame=num_frame, num_joints=num_joints, in_chans=in_chans, embed_dim_ratio=embed_dim_ratio,\n                        depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                        drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=drop_path_rate,\n                        num_classes =num_classes)\n\n# Move the model to the device\nmodel = model.to(device)\n\n#Set the optimizer and learning rate\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Set the number of training epochs\nnum_epochs = 10\n#print(num_epochs)\n#print(num_classes)\nunique_labels = set(train_dataset.labels)\n\n# Create a label encoder\nlabel_encoder = LabelEncoder()\n\n# Fit label encoder on the training labels\nlabel_encoder.fit(train_dataset.labels)\n\n# Convert training labels to numerical values\ntrain_labels_encoded = label_encoder.transform(train_dataset.labels)\n\n# Convert the encoded labels to tensors\ntrain_labels_tensor = torch.tensor(train_labels_encoded, dtype=torch.long).to(device)\n\n# Set the threshold variable\nthreshold = None\n\n# Create an instance of the TripletLoss\ncompute_triplet_loss = TripletLoss()\n\n\n# Training loop\nfor epoch in range(num_epochs):\n    #print(\"epoch\")\n    #print(epoch)\n    model.train()  # Set the model to training mode\n    total_triplet_loss = 0.0\n\n    # Initialize an empty list to store anchor embeddings\n    all_anchor_embeddings_tsne = []\n    all_pos_embeddings_tsne = []\n    all_neg_embeddings_tsne = []\n\n    for triplet_sample, labels in train_dataloader:  # Load triplets\n\n        triplet_sample = triplet_sample.to(device)\n\n        labels_encoded = label_encoder.transform(labels)  # Convert current batch labels to numerical values\n        labels_tensor = torch.tensor(labels_encoded, dtype=torch.long).to(device)\n\n        # Forward pass for the entire triplet sample\n        triplet_embeddings = model(triplet_sample)  # Shape: (bs, 3, f)\n\n\n        anchor_embeddings = triplet_embeddings[:, 0, :]  # Select the anchor embeddings\n        positive_embeddings = triplet_embeddings[:, 1, :]  # Select the positive embeddings\n        negative_embeddings = triplet_embeddings[:, 2, :]  # Select the negative embeddings\n\n        # Compute triplet loss\n        triplet_loss = compute_triplet_loss(anchor_embeddings, positive_embeddings, negative_embeddings)\n\n        optimizer.zero_grad()\n        triplet_loss.backward()\n        optimizer.step()\n\n        total_triplet_loss += triplet_loss.item()\n\n        anchor_embeddings_tsne = anchor_embeddings.view(-1, 1, emb_dim)\n        pos_embeddings_tsne = positive_embeddings.view(-1, 1, emb_dim)\n        neg_embeddings_tsne = negative_embeddings.view(-1, 1, emb_dim)\n\n        # Append anchor_embeddings to the list\n\n        all_anchor_embeddings_tsne.append(anchor_embeddings_tsne)\n        all_pos_embeddings_tsne.append(pos_embeddings_tsne)\n        all_neg_embeddings_tsne.append(neg_embeddings_tsne)\n\n\n    average_triplet_loss = total_triplet_loss / len(train_dataloader)\n    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Triplet Loss: {average_triplet_loss:.4f}')\n\n\nall_anchor_embeddings_tsne = torch.cat(all_anchor_embeddings_tsne, dim=0)\nall_pos_embeddings_tsne = torch.cat(all_pos_embeddings_tsne, dim=0)\nall_neg_embeddings_tsne = torch.cat(all_neg_embeddings_tsne, dim=0)\n\n# Save the anchor embeddings to a file in the working directory\ntorch.save(all_anchor_embeddings_tsne, 'anchor_embeddings.pth')\ntorch.save(all_pos_embeddings_tsne, 'positive_embeddings.pth')\ntorch.save(all_neg_embeddings_tsne, 'negative_embeddings.pth')\n\n# Save the trained model to the working directory\ntorch.save(model.state_dict(), 'gesture_comparison_pose_transformer_model.pth')\n\npos_distances = compute_distance(all_anchor_embeddings_tsne, all_pos_embeddings_tsne)\n\nneg_distances = compute_distance(all_anchor_embeddings_tsne, all_neg_embeddings_tsne)\n\n\ncos_pos_distances = compute_distance(all_anchor_embeddings_tsne, all_pos_embeddings_tsne, distance_metric='cosine')\n\ncos_neg_distances = compute_distance(all_anchor_embeddings_tsne, all_neg_embeddings_tsne, distance_metric='cosine')\n\n\nminimum_distance = min(torch.min(pos_distances), torch.min(neg_distances)).item()\nmaximum_distance = max(torch.max(pos_distances), torch.max(neg_distances)).item()\n\ncos_minimum_distance = min(torch.min(cos_pos_distances), torch.min(cos_neg_distances)).item()\ncos_maximum_distance = max(torch.max(cos_pos_distances), torch.max(cos_neg_distances)).item()\n\n\npos_distances_np = pos_distances.detach().cpu().numpy()\n\nneg_distances_np = neg_distances.detach().cpu().numpy()\n\ncos_pos_distances_np = cos_pos_distances.detach().cpu().numpy()\n\ncos_neg_distances_np = cos_neg_distances.detach().cpu().numpy()\n\nsns.set_style(\"whitegrid\")  # Set plot style\n\n# Calculate the minimum and maximum values across both datasets\nmin_distance = min(np.min(pos_distances_np), np.min(neg_distances_np))\nmax_distance = max(np.max(pos_distances_np), np.max(neg_distances_np))\n\n# Create histograms with the same bin width for both datasets\nplt.figure(figsize=(8, 6))\npos_counts, pos_bins, _ = plt.hist(pos_distances_np, bins=100, range=(min_distance, max_distance), alpha=0.5, color=\"blue\", label=\"Anchor-Positive Distances\")\nneg_counts, neg_bins, _ = plt.hist(neg_distances_np, bins=100, range=(min_distance, max_distance), alpha=0.5, color=\"red\", label=\"Anchor-Negative Distances\")\n\nplt.xlabel(\"Distance\")\nplt.ylabel(\"Number of Samples\")\nplt.title(\"Histogram of Anchor-Positive and Anchor-Negative Euclidean Distances \")\n\n# Find the intersection point (threshold)\ndef find_intersection1(x):\n    pos_indices = np.searchsorted(pos_bins, x, side='right') - 1\n    neg_indices = np.searchsorted(neg_bins, x, side='right') - 1\n    pos_interpolated = np.interp(x, pos_bins[:-1], pos_counts)\n    neg_interpolated = np.interp(x, neg_bins[:-1], neg_counts)\n    return pos_interpolated - neg_interpolated\n\nminimum_distance = max(min(pos_distances_np), min(neg_distances_np))\nmaximum_distance = min(max(pos_distances_np), max(neg_distances_np))\n\n\n# Calculate the intersection point within the defined range\neuclidean_intersection_point = brentq(find_intersection1, minimum_distance, maximum_distance)\n\n#print(f\"Euclidean Intersection Point (Threshold): {euclidean_intersection_point:.2f}\")\n\nplt.axvline(x=euclidean_intersection_point, color='green', linestyle='--', label=f\"Threshold: {euclidean_intersection_point:.2f}\")\nplt.legend()\n\n# Save the histogram plot as an image in the working directory\nplt.savefig('euclidean_threshold_plot.png', dpi=300)  # Save as PNG with high resolution\nplt.close()  # Close the plot to release memory\n\n\n\n# Calculate the minimum and maximum values across both datasets\nmin_distance = min(np.min(cos_pos_distances_np), np.min(cos_neg_distances_np))\nmax_distance = max(np.max(cos_pos_distances_np), np.max(cos_neg_distances_np))\n\n# Create histograms with the same bin width for both datasets\nplt.figure(figsize=(8, 6))\npos_counts, pos_bins, _ = plt.hist(cos_pos_distances_np, bins=100, range=(min_distance, max_distance), alpha=0.5, color=\"blue\", label=\"Anchor-Positive Distances\")\nneg_counts, neg_bins, _ = plt.hist(cos_neg_distances_np, bins=100, range=(min_distance, max_distance), alpha=0.5, color=\"red\", label=\"Anchor-Negative Distances\")\n\nplt.xlabel(\"Distance\")\nplt.ylabel(\"Number of Samples\")\nplt.title(\"Histogram of Anchor-Positive and Anchor-Negative Cosine Distances \")\n\n# Find the intersection point (threshold)\ndef find_intersection1(x):\n    pos_indices = np.searchsorted(pos_bins, x, side='right') - 1\n    neg_indices = np.searchsorted(neg_bins, x, side='right') - 1\n    pos_interpolated = np.interp(x, pos_bins[:-1], pos_counts)\n    neg_interpolated = np.interp(x, neg_bins[:-1], neg_counts)\n    return pos_interpolated - neg_interpolated\n\nminimum_distance = max(min(cos_pos_distances_np), min(cos_neg_distances_np))\nmaximum_distance = min(max(cos_pos_distances_np), max(cos_neg_distances_np))\n\n\n#Calculate the intersection point within the defined range\ncosine_intersection_point = brentq(find_intersection1, minimum_distance, maximum_distance)\n\n#print(f\"Cosine Intersection Point (Threshold): {cosine_intersection_point:.2f}\")\n\nplt.axvline(x=cosine_intersection_point, color='green', linestyle='--', label=f\"Threshold: {cosine_intersection_point:.2f}\")\nplt.legend()\n\n# Save the histogram plot as an image in the working directory\nplt.savefig('cosine_threshold_plot.png', dpi=300)  # Save as PNG with high resolution\nplt.close()  # Close the plot to release memory\n\nthreshold = euclidean_intersection_point\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Initialize lists to store predictions and targets\ntest_predictions = []\ntest_targets = []\n\n# Initialize an empty list to store anchor embeddings\nall_anchor_embeddings = []\nall_pos_embeddings = []\nall_neg_embeddings = []\n\n\n# Testing loop\nwith torch.no_grad():\n    for triplet_sample, labels in test_dataloader:  # Load triplets and labels\n        triplet_sample = triplet_sample.to(device)\n\n        labels_encoded = label_encoder.transform(labels)  # Convert current batch labels to numerical values\n        labels_tensor = torch.tensor(labels_encoded, dtype=torch.long).to(device)\n\n        # Forward pass for the entire triplet sample\n        triplet_embeddings = model(triplet_sample)  # Shape: (bs, 3, f)\n\n        # Split the embeddings into anchor, positive, and negative parts\n        anchor_embeddings = triplet_embeddings[:, 0, :]  # Select the anchor embeddings\n        positive_embeddings = triplet_embeddings[:, 1, :]  # Select the positive embeddings\n        negative_embeddings = triplet_embeddings[:, 2, :]  # Select the negative embeddings\n\n        anchor_embeddings = anchor_embeddings.view(-1, 1, emb_dim)\n        positive_embeddings = positive_embeddings.view(-1, 1, emb_dim)\n        negative_embeddings = negative_embeddings.view(-1, 1, emb_dim)\n\n        # Append anchor_embeddings to the list\n        all_anchor_embeddings.append(anchor_embeddings)\n        all_pos_embeddings.append(positive_embeddings)\n        all_neg_embeddings.append(negative_embeddings)\n\n        test_targets.append(labels_tensor)\n\n\n# Concatenate all anchor embeddings into a single tensor\nall_anchor_embeddings_tsne = torch.cat(all_anchor_embeddings, dim=0)\nanchor_embeddings = all_anchor_embeddings_tsne\nall_pos_embeddings_tsne = torch.cat(all_pos_embeddings, dim=0)\nall_neg_embeddings_tsne = torch.cat(all_neg_embeddings, dim=0)\n\n\n\npos_distances = compute_distance(all_anchor_embeddings_tsne, all_pos_embeddings_tsne)\n\nneg_distances = compute_distance(all_anchor_embeddings_tsne, all_neg_embeddings_tsne)\n\n\ncos_pos_distances = compute_distance(all_anchor_embeddings_tsne, all_pos_embeddings_tsne, distance_metric='cosine')\n\ncos_neg_distances = compute_distance(all_anchor_embeddings_tsne, all_neg_embeddings_tsne, distance_metric='cosine')\n\n\n\npos_distances_np = pos_distances.detach().cpu().numpy()\n\nneg_distances_np = neg_distances.detach().cpu().numpy()\n\ncos_pos_distances_np = cos_pos_distances.detach().cpu().numpy()\n\ncos_neg_distances_np = cos_neg_distances.detach().cpu().numpy()\n\n\n\n# Calculate accuracy\naccuracy = calculate_accuracy(pos_distances_np, neg_distances_np, threshold)\n\naccuracy_cos = calculate_accuracy(cos_pos_distances_np, cos_neg_distances_np, threshold)\n\n\nprint(f'Accuracy: {accuracy * 100:.2f}%')\n\n\ntargets = torch.cat(test_targets, dim=0)\n\n\ncustom_colors = ['#a26989', '#924ff1', '#6029e8', '#f0215b', '#8da6ae', '#fc8cd9', '#5cc8ff', '#d2cd29', '#fc85a8', '#a55e73', '#89b38f', '#3c2b60', '#27ba26', '#5b274d', '#84421a', '#c142c0', '#97bf08', '#fe978e', '#c3dd15', '#5b7ff6', '#fd979c', '#13a66c', '#ba2b12', '#963409', '#d155d4', '#86ad4d', '#da12ba', '#0273dd', '#998723', '#d7d618', '#213eb7', '#d291c8', '#86690b', '#caf200', '#c546e2', '#0ab739', '#8b0624', '#90acc7', '#944934', '#565835', '#50efbb', '#905ee0', '#f3a7ea', '#bbd9f9', '#a452d9']\n\n\n# Convert anchor_embeddings to numpy array\nanchor_embeddings_np = anchor_embeddings.cpu().numpy()  # Assuming anchor_embeddings is a PyTorch tensor\n\n#Flatten the anchor embeddings to a 2D array\nflattened_anchor_embeddings = anchor_embeddings.view(anchor_embeddings.size(0), -1).cpu().numpy()\n\n\n# Get the labels for the tsne embeddings\ntsne_labels = targets.cpu().numpy()\n\nperplexity_values = [5, 10, 20, 30, 50, 100]\niteration_values = [250, 500, 1000, 2000]\n\nplt.figure(figsize=(15, 15))\nplot_num = 1\nnum= 1\n\ncmap = ListedColormap(custom_colors[:len(np.unique(tsne_labels))])\n\nfor perplexity in perplexity_values:\n    for iterations in iteration_values:\n        tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=iterations, random_state=42)\n        tsne_embeddings = tsne.fit_transform(flattened_anchor_embeddings)\n        plt.subplot(len(perplexity_values), len(iteration_values), plot_num)\n        plt.scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1], c=tsne_labels, cmap=cmap , s=0.5)\n        plt.title(f'Perplexity: {perplexity}, Iterations: {iterations}')\n        plt.show()\n        plt.xticks([])\n        plt.yticks([])\n        plt.savefig('tsne_plot' + str(num) + '.png', dpi=300)\n        plot_num += 1\n        num = num+1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T18:01:50.232131Z","iopub.execute_input":"2025-08-16T18:01:50.232359Z","execution_failed":"2025-08-16T20:13:44.287Z"}},"outputs":[{"name":"stdout","text":"2\n45\nepoch\n0\nEpoch [1/2], Average Triplet Loss: 0.5145\nepoch\n1\nEpoch [2/2], Average Triplet Loss: 0.4237\nEuclidean Intersection Point (Threshold): 2.08\nCosine Intersection Point (Threshold): 0.01\nAccuracy: 71.13%\nAccuracy cos: 50.00%\n","output_type":"stream"}],"execution_count":null}]}