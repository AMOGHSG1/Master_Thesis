{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport logging\nfrom functools import partial\nfrom collections import OrderedDict\nfrom einops import rearrange, repeat\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.models.helpers import load_pretrained\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nfrom timm.models.registry import register_model\n\n\nimport os\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\nclass GestureClassification(nn.Module):\n    def __init__(self, num_frame=101, num_joints=21, in_chans=3, embed_dim_ratio=32, depth=4,\n                 num_heads=8, mlp_ratio=2., qkv_bias=True, qk_scale=None,\n                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.2,  norm_layer=None, num_classes = 0):\n        \"\"\"    ##########hybrid_backbone=None, representation_size=None,\n        Args:\n            num_frame (int, tuple): input frame number\n            num_joints (int, tuple): joints number\n            in_chans (int): number of input channels, 2D joints have 2 channels: (x,y)\n            embed_dim_ratio (int): embedding dimension ratio\n            depth (int): depth of transformer\n            num_heads (int): number of attention heads\n            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n            qkv_bias (bool): enable bias for qkv if True\n            qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n            drop_rate (float): dropout rate\n            attn_drop_rate (float): attention dropout rate\n            drop_path_rate (float): stochastic depth rate\n            norm_layer: (nn.Module): normalization layer\n        \"\"\"\n        super().__init__()\n\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n        embed_dim = embed_dim_ratio * num_joints   #### temporal embed_dim is num_joints * spatial embedding dim ratio\n        out_dim = num_classes\n\n        ### spatial patch embedding\n        self.Spatial_patch_to_embedding = nn.Linear(in_chans, embed_dim_ratio)\n        self.Spatial_pos_embed = nn.Parameter(torch.zeros(1, num_joints, embed_dim_ratio))\n\n        self.Temporal_pos_embed = nn.Parameter(torch.zeros(1, num_frame, embed_dim))\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n\n        self.Spatial_blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim_ratio, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n            for i in range(depth)])\n\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n            for i in range(depth)])\n\n        self.Spatial_norm = norm_layer(embed_dim_ratio)\n        self.Temporal_norm = norm_layer(embed_dim)\n\n        ####### A easy way to implement weighted mean\n        self.weighted_mean = torch.nn.Conv1d(in_channels=num_frame, out_channels=1, kernel_size=1)\n\n        self.head = nn.Sequential(\n            nn.LayerNorm(embed_dim),\n            nn.Linear(embed_dim , out_dim),\n        )\n\n\n    def Spatial_forward_features(self, x):\n        b, _, f, p = x.shape  ##### b is batch size, f is number of frames, p is number of joints\n        x = rearrange(x, 'b c f p  -> (b f) p  c', )\n\n\n        x = self.Spatial_patch_to_embedding(x)\n        x += self.Spatial_pos_embed\n        x = self.pos_drop(x)\n\n        for blk in self.Spatial_blocks:\n            x = blk(x)\n\n        x = self.Spatial_norm(x)\n        x = rearrange(x, '(b f) w c -> b f (w c)', f=f)\n        return x\n\n    def forward_features(self, x):\n        b  = x.shape[0]\n        x += self.Temporal_pos_embed\n        x = self.pos_drop(x)\n        for blk in self.blocks:\n            x = blk(x)\n\n        x = self.Temporal_norm(x)\n        ##### x size [b, f, emb_dim], then take weighted mean on frame dimension, we only predict 3D pose of the center frame\n        x = self.weighted_mean(x)\n        x = x.view(b, 1, -1)\n        return x\n\n\n    def forward(self, x):\n        x = x.permute(0, 3, 1, 2)\n        b, _, _, p = x.shape\n        x = self.Spatial_forward_features(x)\n        x = self.forward_features(x)\n        x = self.head(x)\n        x = x.view(b, num_classes)\n\n        return x\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MyCustomDataset(Dataset):\n    def __init__(self, root_dir, mode='train', sliding_window_size=101):\n        self.root_dir = root_dir\n        self.subjects = os.listdir(root_dir)\n        self.frame_data = []  # Stores the information of each frame\n        self.labels = [] # Stores the labels of each frame\n        self.indexvalues = []\n        self.sliding_window_size = sliding_window_size\n        self._process_data(mode)\n\n    def __len__(self):\n        return len(self.frame_data)\n\n\n    def __getitem__(self, index):\n        center_frame = self.frame_data[index]\n        label = self.labels[index]\n        skeleton_number = center_frame['skeleton_number']\n\n        skeleton_start_index = self.indexvalues[skeleton_number]['start_index']\n        skeleton_end_index = self.indexvalues[skeleton_number]['end_index']\n\n        start_index = index - self.sliding_window_size // 2\n        end_index = index + self.sliding_window_size // 2\n\n        if (start_index < skeleton_start_index):\n          start_index = skeleton_start_index\n\n        else:\n          start_index = max(start_index, 0)\n\n        if (end_index > skeleton_end_index):\n          end_index = skeleton_end_index\n\n        t = []\n\n        for i in range(start_index, end_index + 1):\n          frame_data = self.frame_data[i]\n          joint_coordinates = frame_data['joint_coordinates']\n          t.append(joint_coordinates)\n\n        while len(t) < self.sliding_window_size:\n          t.append([0.0] * 63)  # Assuming 21 joints with (x, y, z) coordinates\n\n        t = torch.tensor(t, dtype=torch.float32)\n        t = t.view(self.sliding_window_size, 21, 3)\n\n        return t, label\n\n\n    def _process_data(self, mode):\n        frame_counter = 0  # Counter for sequential frame numbering\n        skeleton_temp=0\n        indices = []\n\n\n        for subject in self.subjects:\n            subject_dir = os.path.join(self.root_dir, subject)\n\n            if mode == 'train':\n                data_dir = os.path.join(subject_dir, 'TrainingData')\n            elif mode == 'test':\n                data_dir = os.path.join(subject_dir, 'TestingData')\n            else:\n                raise ValueError(\"Invalid mode. Mode should be 'train' or 'test'.\")\n\n            action_folders = os.listdir(subject_dir)\n\n            for action_folder in action_folders:\n                action_dir = os.path.join(subject_dir, action_folder)\n                number_folders = os.listdir(action_dir)\n\n                for number_folder in number_folders:\n                    number_dir = os.path.join(action_dir, number_folder)\n                    skeleton_file = os.path.join(number_dir, \"skeleton.txt\")\n\n                    # Read and process the skeleton.txt file to extract frame-level data\n                    frames, labels , start , end= self._read_skeleton_file(skeleton_file, frame_counter, action_folder , skeleton_temp)\n                    indices.append({\n                    'start_index': start,\n                    'end_index': end,\n\n                    })\n                    self.frame_data.extend(frames)\n                    self.labels.extend(labels)\n\n                    # Update the frame counter based on the number of frames in the current skeleton file\n                    frame_counter += len(frames)\n                    skeleton_temp = skeleton_temp + 1\n\n        self.indexvalues.extend(indices)\n\n\n\n    def _read_skeleton_file(self, skeleton_file, frame_counter, action , skeleton_temp):\n        frames = []\n        labels = []\n        start_ind = frame_counter\n        temp_index = 0\n\n        # Read and parse the skeleton.txt file to extract frame-level data\n        with open(skeleton_file, 'r') as f:\n            for line in f:\n                # Parse the line and extract frame number, joint coordinates, and label\n                frame_info = line.strip().split(' ')\n                frame_number = frame_counter + int(frame_info[0])\n                joint_coordinates = [float(coord) for coord in frame_info[1:]]\n                label = action\n                end_ind = frame_number\n\n                # Append the frame data and label to the respective lists\n                frames.append({\n                    'frame_number': frame_number,\n                    'joint_coordinates': joint_coordinates,\n                    'skeleton_number' : skeleton_temp,\n\n                })\n                labels.append(label)\n                temp_index = frame_number\n\n        return frames, labels, start_ind, temp_index","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n# Set the root directory of your dataset\ntrain_root_dir = '/kaggle/input/fphadataset/FPHADataset/TrainingData'\ntest_root_dir = '/kaggle/input/fphadataset/FPHADataset/TestingData'\n\n# Create an instance of the training dataset\ntrain_dataset = MyCustomDataset(train_root_dir, mode='train')\n\n# Create an instance of the testing dataset\ntest_dataset = MyCustomDataset(test_root_dir, mode='test')\n\n# Set the batch size\nbatch_size = 32\n\n# Create data loaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\ntrain_dataset_length = len(train_dataset)\n\ntest_dataset_length = len(test_dataset)\n\n\n# Set the number of output classes\nnum_classes = 45 # Number of classes in dataset\n\n# Set the hyperparameters\n#Sliding window size\nnum_frame = 101\nnum_joints = 21\nin_chans = 3\nembed_dim_ratio = 64\ndepth = 4\nnum_heads = 8\nmlp_ratio = 2.\nqkv_bias = True\nqk_scale = None\ndrop_rate = 0.\nattn_drop_rate = 0.\ndrop_path_rate = 0.2\n\n\n# Create an instance of the PoseTransformer model\nmodel = GestureClassification(num_frame=num_frame, num_joints=num_joints, in_chans=in_chans, embed_dim_ratio=embed_dim_ratio,\n                        depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                        drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=drop_path_rate,\n                        num_classes =num_classes)\n\n# Move the model to the device\nmodel = model.to(device)\n\n# Set the optimizer and learning rate\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Set the loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Set the number of training epochs\nnum_epochs = 10\n\n# Create a label encoder\nlabel_encoder = LabelEncoder()\n\n# Fit label encoder on the training labels\nlabel_encoder.fit(train_dataset.labels)\n\n# Convert training labels to numerical values\ntrain_labels_encoded = label_encoder.transform(train_dataset.labels)\n\n# Convert the encoded labels to tensors\ntrain_labels_tensor = torch.tensor(train_labels_encoded, dtype=torch.long).to(device)\n\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()  # Set the model to training mode\n    train_loss = 0.0\n    train_correct = 0\n\n    for inputs, labels in train_dataloader:\n        inputs = inputs.to(device)\n\n        labels_encoded = label_encoder.transform(labels)  # Convert current batch labels to numerical values\n        labels_tensor = torch.tensor(labels_encoded, dtype=torch.long).to(device)\n\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs)\n\n        # Compute the loss\n        loss = criterion(outputs, labels_tensor)\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n\n        # Compute the training accuracy\n        _, predicted = torch.max(outputs, 1)\n        train_correct += (predicted == labels_tensor).sum().item()\n\n        # Update the training loss\n        train_loss += loss.item() * inputs.size(0)\n\n    # Compute the average training loss and accuracy\n    train_loss = train_loss / len(train_dataset)\n    train_accuracy = train_correct / len(train_dataset)\n\n    # Print the training loss and accuracy for each epoch\n    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n\n    # Validation loop\n    model.eval()  # Set the model to evaluation mode\n    test_loss = 0.0\n    test_correct = 0\n    test_predictions = []\n    test_targets = []\n\n\n\n    with torch.no_grad():\n        for inputs, labels in test_dataloader:\n            inputs = inputs.to(device)\n\n            labels_encoded = label_encoder.transform(labels)  # Convert current batch labels to numerical values\n            labels_tensor = torch.tensor(labels_encoded, dtype=torch.long).to(device)\n\n\n            # Forward pass\n            outputs = model(inputs)\n\n            # Compute the loss\n            loss = criterion(outputs, labels_tensor)\n\n            # Compute the testing accuracy\n            _, predicted = torch.max(outputs, 1)\n            test_correct += (predicted == labels_tensor).sum().item()\n\n            # Update the testing loss\n            test_loss += loss.item() * inputs.size(0)\n\n            # Collect predictions and targets for computing metrics\n            test_predictions.extend(predicted.cpu().numpy())\n            test_targets.extend(labels_encoded)\n\n    # Compute the average testing loss and accuracy\n    test_loss = test_loss / len(test_dataset)\n    test_accuracy = test_correct / len(test_dataset)\n\n    # Print the testing loss and accuracy for each epoch\n    print(f'Epoch [{epoch + 1}/{num_epochs}], Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n\n\n# Save the trained model\ntorch.save(model.state_dict(), '81_pose_transformer_model.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}